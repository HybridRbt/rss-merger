#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import requests
from datetime import datetime, timezone, timedelta
import xml.etree.ElementTree as ET
import hashlib
import re
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import html

class DateFilteredRSSAggregator:
    def __init__(self, output_file='final_rss.xml', html_file='final_daily_news.html'):
        # RSSæºåˆ—è¡¨ï¼ˆå·²ç§»é™¤å°‘æ•°æ´¾ï¼‰
        self.rss_feeds = [
            'https://www.ifanr.com/feed',  # çˆ±èŒƒå„¿
            'https://www.geekpark.net/rss'  # æå®¢å…¬å›­
        ]
        
        self.output_file = output_file
        self.html_file = html_file
        
        # ä¸­å›½æ ‡å‡†æ—¶é—´ (CST, UTC+8)
        self.cst_timezone = timezone(timedelta(hours=8))
        
        # è·å–å½“å‰CSTæ—¶é—´
        self.today_cst = datetime.now(self.cst_timezone)
        self.today_date_str = self.today_cst.strftime('%Y-%m-%d')
        
        print(f"å½“å‰CSTæ—¶é—´: {self.today_cst}")
        print(f"ç­›é€‰æ—¥æœŸ: {self.today_date_str}")

    def _is_today_cst(self, pub_date):
        """æ£€æŸ¥æ–‡ç« å‘å¸ƒæ—¥æœŸæ˜¯å¦ä¸ºä»Šå¤©ï¼ˆCSTæ—¶é—´ï¼‰"""
        try:
            if pub_date.tzinfo is None:
                # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºCST
                pub_date = pub_date.replace(tzinfo=self.cst_timezone)
            else:
                # è½¬æ¢ä¸ºCSTæ—¶é—´
                pub_date = pub_date.astimezone(self.cst_timezone)
            
            pub_date_str = pub_date.strftime('%Y-%m-%d')
            return pub_date_str == self.today_date_str
        except Exception as e:
            print(f"æ—¥æœŸè§£æé”™è¯¯: {e}")
            return False

    def _extract_text_features(self, title, description):
        """æå–æ–‡æœ¬ç‰¹å¾ç”¨äºèšç±»"""
        # æ¸…ç†HTMLæ ‡ç­¾
        clean_desc = re.sub(r'<[^>]+>', '', description) if description else ''
        
        # åˆå¹¶æ ‡é¢˜å’Œæè¿°
        text = f"{title} {clean_desc}"
        
        # æå–å…³é”®è¯ï¼ˆç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼‰
        keywords = re.findall(r'[\u4e00-\u9fa5]+', text)
        features = ' '.join(keywords)
        
        return {
            'text': features,
            'title': title,
            'description': clean_desc
        }

    def _cluster_and_filter_topics(self, articles):
        """èšç±»ç›¸ä¼¼è¯é¢˜å¹¶å»é‡"""
        if len(articles) <= 1:
            return articles, []

        # æå–æ–‡æœ¬ç‰¹å¾
        features_list = []
        for article in articles:
            features = self._extract_text_features(article['title'], article.get('description', ''))
            features_list.append(features)
            article['features'] = features

        # ä½¿ç”¨TF-IDFè¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
        texts = [f['text'] for f in features_list]
        if not any(texts):  # å¦‚æœæ‰€æœ‰æ–‡æœ¬éƒ½ä¸ºç©º
            return articles, []

        try:
            vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # èšç±»ç›¸ä¼¼æ–‡ç« ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼0.3ï¼‰
            clusters = []
            used = set()
            
            for i in range(len(articles)):
                if i in used:
                    continue
                    
                cluster = [i]
                used.add(i)
                
                for j in range(i + 1, len(articles)):
                    if j not in used and similarity_matrix[i][j] > 0.3:
                        cluster.append(j)
                        used.add(j)
                
                clusters.append(cluster)

            # å¤„ç†æ¯ä¸ªèšç±»
            merged_topics = []
            unique_topics = []

            for cluster in clusters:
                cluster_articles = [articles[i] for i in cluster]
                
                if len(cluster_articles) == 1:
                    # å•ç‹¬çš„æ–‡ç« ï¼Œç›´æ¥æ·»åŠ 
                    unique_topics.append(cluster_articles[0])
                else:
                    # å¤šç¯‡ç›¸ä¼¼æ–‡ç« ï¼Œè¿›è¡Œåˆå¹¶
                    merged_topic = self._merge_topic_cluster(cluster_articles)
                    if merged_topic:
                        merged_topics.append(merged_topic)

            print(f"è¯é¢˜èšç±»ä¸è¿‡æ»¤åï¼Œä¿ç•™ {len(merged_topics)} ä¸ªä¸»è¦è¯é¢˜å’Œ {len(unique_topics)} ä¸ªç‹¬ç‰¹è¯é¢˜")
            return merged_topics, unique_topics

        except Exception as e:
            print(f"èšç±»è¿‡ç¨‹å‡ºé”™: {e}")
            return articles, []

    def _merge_topic_cluster(self, cluster_articles):
        """åˆå¹¶åŒä¸€è¯é¢˜ä¸‹çš„å¤šç¯‡æ–‡ç« """
        if not cluster_articles:
            return None

        # é€‰æ‹©æœ€é•¿çš„æ–‡ç« ä½œä¸ºä»£è¡¨ï¼Œå¹¶æ ‡è®°ä¸ºæœ‰å¾…å¤„ç†çš„å°‘æ•°æ´¾æ–‡ç« 
        representative = max(cluster_articles, key=lambda t: len(t.get('features', {}).get('text', '')))
        
        # åˆå¹¶æ‰€æœ‰æ–‡ç« çš„ä¿¡æ¯
        all_sources = []
        all_links = []
        
        for article in cluster_articles:
            source = article.get('source', 'æœªçŸ¥æ¥æº')
            link = article.get('link', '')
            
            if source not in all_sources:
                all_sources.append(source)
            if link and link not in all_links:
                all_links.append(link)

        # åˆ›å»ºåˆå¹¶åçš„æ–‡ç« 
        merged_article = {
            'title': representative['title'],
            'description': representative.get('description', ''),
            'link': representative.get('link', ''),
            'pub_date': representative.get('pub_date'),
            'source': ', '.join(all_sources),
            'guid': representative.get('guid', ''),
            'merged_sources': len(cluster_articles),
            'all_links': all_links
        }

        return merged_article

    def fetch_and_filter_articles(self):
        """è·å–å¹¶ç­›é€‰ä»Šæ—¥æ–‡ç« """
        all_articles = []
        
        for feed_url in self.rss_feeds:
            try:
                print(f"æ­£åœ¨è·å–RSSæº: {feed_url}")
                response = requests.get(feed_url, timeout=30)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                
                if feed.bozo:
                    print(f"è­¦å‘Š: RSSæºå¯èƒ½æœ‰æ ¼å¼é—®é¢˜: {feed_url}")
                
                source_name = feed.feed.get('title', feed_url)
                print(f"RSSæºæ ‡é¢˜: {source_name}")
                
                today_articles = []
                for entry in feed.entries:
                    try:
                        # è§£æå‘å¸ƒæ—¥æœŸ
                        pub_date = None
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_date = datetime(*entry.published_parsed[:6])
                        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                            pub_date = datetime(*entry.updated_parsed[:6])
                        
                        if pub_date and self._is_today_cst(pub_date):
                            article = {
                                'title': entry.get('title', 'æ— æ ‡é¢˜'),
                                'description': entry.get('description', ''),
                                'link': entry.get('link', ''),
                                'pub_date': pub_date,
                                'source': source_name,
                                'guid': entry.get('id', entry.get('link', ''))
                            }
                            today_articles.append(article)
                    
                    except Exception as e:
                        print(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ä» {source_name} è·å–åˆ° {len(today_articles)} ç¯‡ä»Šæ—¥æ–‡ç« ")
                all_articles.extend(today_articles)
                
            except Exception as e:
                print(f"è·å–RSSæºå¤±è´¥ {feed_url}: {e}")
                continue
        
        print(f"æ€»å…±è·å–åˆ° {len(all_articles)} ç¯‡ä»Šæ—¥æ–‡ç« ")
        return all_articles

    def generate_rss(self, merged_topics, unique_topics):
        """ç”ŸæˆRSSæ–‡ä»¶"""
        # åˆ›å»ºRSSæ ¹å…ƒç´ 
        rss = ET.Element('rss')
        rss.set('version', '2.0')
        rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
        
        channel = ET.SubElement(rss, 'channel')
        
        # RSSé¢‘é“ä¿¡æ¯
        title = ET.SubElement(channel, 'title')
        title.text = 'æ¯æ—¥ç§‘æŠ€æ–°é—»èšåˆ'
        
        description = ET.SubElement(channel, 'description')
        description.text = f'èšåˆçˆ±èŒƒå„¿ã€æå®¢å…¬å›­çš„æ¯æ—¥ç§‘æŠ€æ–°é—» - {self.today_date_str}'
        
        link = ET.SubElement(channel, 'link')
        github_pages_url = 'https://hybridrbt.github.io/rss-merger'
        link.text = github_pages_url
        
        language = ET.SubElement(channel, 'language')
        language.text = 'zh-CN'
        
        # æ·»åŠ å½“å‰CSTæ—¶é—´ä½œä¸ºlastBuildDate
        last_build_date = ET.SubElement(channel, 'lastBuildDate')
        last_build_date.text = self.today_cst.strftime('%a, %d %b %Y %H:%M:%S %z')
        
        # æ·»åŠ pubDate
        pub_date_elem = ET.SubElement(channel, 'pubDate')
        pub_date_elem.text = self.today_cst.strftime('%a, %d %b %Y %H:%M:%S %z')
        
        # æ·»åŠ atom:linkè‡ªå¼•ç”¨
        atom_link = ET.SubElement(channel, 'atom:link')
        atom_link.set('href', f'{github_pages_url}/{self.output_file}')
        atom_link.set('rel', 'self')
        atom_link.set('type', 'application/rss+xml')
        
        # æ·»åŠ generatorä¿¡æ¯
        generator = ET.SubElement(channel, 'generator')
        generator.text = 'æ–°é—»èšåˆå™¨'
        
        # æ·»åŠ sourceä¿¡æ¯
        source = ET.SubElement(channel, 'source')
        source.set('url', github_pages_url)
        source.text = 'æ–°é—»èšåˆå™¨'
        
        # ç”Ÿæˆèšåˆæ–‡ç« çš„å”¯ä¸€GUIDï¼ˆåŸºäºCSTæ—¥æœŸï¼‰
        date_hash = hashlib.md5(self.today_date_str.encode('utf-8')).hexdigest()[:8]
        aggregated_guid = f"aggregated-news-{self.today_date_str}-{date_hash}"
        
        # æ·»åŠ èšåˆæ–‡ç« 
        if merged_topics or unique_topics:
            item = ET.SubElement(channel, 'item')
            
            item_title = ET.SubElement(item, 'title')
            item_title.text = f'æ¯æ—¥ç§‘æŠ€æ–°é—»èšåˆ - {self.today_date_str}'
            
            item_description = ET.SubElement(item, 'description')
            
            # ç”ŸæˆHTMLå†…å®¹
            html_content = self._generate_html_content(merged_topics, unique_topics)
            item_description.text = f'<![CDATA[{html_content}]]>'
            
            item_link = ET.SubElement(item, 'link')
            item_link.text = f'{github_pages_url}/{self.html_file}'
            
            item_guid = ET.SubElement(item, 'guid')
            item_guid.set('isPermaLink', 'false')
            item_guid.text = aggregated_guid
            
            item_pub_date = ET.SubElement(item, 'pubDate')
            item_pub_date.text = self.today_cst.strftime('%a, %d %b %Y %H:%M:%S %z')
            
            item_source = ET.SubElement(item, 'source')
            item_source.set('url', github_pages_url)
            item_source.text = 'æ–°é—»èšåˆå™¨'
        
        # å†™å…¥RSSæ–‡ä»¶
        tree = ET.ElementTree(rss)
        ET.indent(tree, space="  ", level=0)
        tree.write(self.output_file, encoding='utf-8', xml_declaration=True)
        
        print(f"RSSæ–‡ä»¶å·²ç”Ÿæˆ: {self.output_file}")

    def _generate_html_content(self, merged_topics, unique_topics):
        """ç”ŸæˆHTMLå†…å®¹"""
        html_parts = []
        html_parts.append('<!DOCTYPE html>')
        html_parts.append('<html lang="zh-CN">')
        html_parts.append('<head>')
        html_parts.append('<meta charset="UTF-8">')
        html_parts.append('<meta name="viewport" content="width=device-width, initial-scale=1.0">')
        html_parts.append(f'<title>æ¯æ—¥ç§‘æŠ€æ–°é—»èšåˆ - {self.today_date_str}</title>')
        html_parts.append('<style>')
        html_parts.append('body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }')
        html_parts.append('h1 { color: #333; border-bottom: 2px solid #007acc; padding-bottom: 10px; }')
        html_parts.append('h2 { color: #007acc; margin-top: 30px; }')
        html_parts.append('.topic { margin-bottom: 25px; padding: 15px; border-left: 4px solid #007acc; background-color: #f8f9fa; }')
        html_parts.append('.topic-title { font-weight: bold; font-size: 1.1em; margin-bottom: 8px; }')
        html_parts.append('.topic-content { margin-bottom: 10px; }')
        html_parts.append('.topic-meta { font-size: 0.9em; color: #666; }')
        html_parts.append('.topic-links { margin-top: 8px; }')
        html_parts.append('.topic-links a { margin-right: 15px; color: #007acc; text-decoration: none; }')
        html_parts.append('.topic-links a:hover { text-decoration: underline; }')
        html_parts.append('.toc { background-color: #f0f0f0; padding: 15px; margin-bottom: 30px; border-radius: 5px; }')
        html_parts.append('.toc h3 { margin-top: 0; }')
        html_parts.append('.toc ul { margin: 0; padding-left: 20px; }')
        html_parts.append('.toc a { color: #007acc; text-decoration: none; }')
        html_parts.append('.toc a:hover { text-decoration: underline; }')
        html_parts.append('</style>')
        html_parts.append('</head>')
        html_parts.append('<body>')
        
        html_parts.append(f'<h1>æ¯æ—¥ç§‘æŠ€æ–°é—»èšåˆ - {self.today_date_str}</h1>')
        html_parts.append(f'<p><strong>æ›´æ–°æ—¶é—´:</strong> {self.today_cst.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S CST")}</p>')
        
        # ç”Ÿæˆç›®å½•
        if merged_topics or unique_topics:
            html_parts.append('<div class="toc">')
            html_parts.append('<h3>ğŸ“‹ ä»Šæ—¥è¯é¢˜ç›®å½•</h3>')
            html_parts.append('<ul>')
            
            topic_index = 1
            for topic in merged_topics:
                anchor = f"topic-{topic_index}"
                title = html.escape(topic['title'][:50] + ('...' if len(topic['title']) > 50 else ''))
                sources_info = f" ({topic['merged_sources']}ç¯‡æ–‡ç« )" if topic.get('merged_sources', 0) > 1 else ""
                html_parts.append(f'<li><a href="#{anchor}">{title}{sources_info}</a></li>')
                topic_index += 1
            
            for topic in unique_topics:
                anchor = f"topic-{topic_index}"
                title = html.escape(topic['title'][:50] + ('...' if len(topic['title']) > 50 else ''))
                html_parts.append(f'<li><a href="#{anchor}">{title}</a></li>')
                topic_index += 1
            
            html_parts.append('</ul>')
            html_parts.append('</div>')
        
        # ä¸»è¦è¯é¢˜ï¼ˆåˆå¹¶åçš„ï¼‰
        if merged_topics:
            html_parts.append('<h2>ğŸ”¥ ä¸»è¦è¯é¢˜</h2>')
            topic_index = 1
            for topic in merged_topics:
                anchor = f"topic-{topic_index}"
                html_parts.append(f'<div class="topic" id="{anchor}">')
                html_parts.append(f'<div class="topic-title">{html.escape(topic["title"])}</div>')
                
                if topic.get('description'):
                    clean_desc = re.sub(r'<[^>]+>', '', topic['description'])
                    if len(clean_desc) > 200:
                        clean_desc = clean_desc[:200] + '...'
                    html_parts.append(f'<div class="topic-content">{html.escape(clean_desc)}</div>')
                
                sources_info = f"æ¥æº: {html.escape(topic.get('source', 'æœªçŸ¥'))}"
                if topic.get('merged_sources', 0) > 1:
                    sources_info += f" (åˆå¹¶äº†{topic['merged_sources']}ç¯‡ç›¸å…³æ–‡ç« )"
                
                html_parts.append(f'<div class="topic-meta">{sources_info}</div>')
                
                if topic.get('all_links'):
                    html_parts.append('<div class="topic-links">')
                    for i, link in enumerate(topic['all_links'][:3]):  # æœ€å¤šæ˜¾ç¤º3ä¸ªé“¾æ¥
                        html_parts.append(f'<a href="{html.escape(link)}" target="_blank">é˜…è¯»åŸæ–‡ {i+1}</a>')
                    html_parts.append('</div>')
                elif topic.get('link'):
                    html_parts.append('<div class="topic-links">')
                    html_parts.append(f'<a href="{html.escape(topic["link"])}" target="_blank">é˜…è¯»åŸæ–‡</a>')
                    html_parts.append('</div>')
                
                html_parts.append('</div>')
                topic_index += 1
        
        # ç‹¬ç‰¹è¯é¢˜
        if unique_topics:
            html_parts.append('<h2>ğŸ“° å…¶ä»–æ–°é—»</h2>')
            for topic in unique_topics:
                anchor = f"topic-{topic_index}"
                html_parts.append(f'<div class="topic" id="{anchor}">')
                html_parts.append(f'<div class="topic-title">{html.escape(topic["title"])}</div>')
                
                if topic.get('description'):
                    clean_desc = re.sub(r'<[^>]+>', '', topic['description'])
                    if len(clean_desc) > 200:
                        clean_desc = clean_desc[:200] + '...'
                    html_parts.append(f'<div class="topic-content">{html.escape(clean_desc)}</div>')
                
                html_parts.append(f'<div class="topic-meta">æ¥æº: {html.escape(topic.get("source", "æœªçŸ¥"))}</div>')
                
                if topic.get('link'):
                    html_parts.append('<div class="topic-links">')
                    html_parts.append(f'<a href="{html.escape(topic["link"])}" target="_blank">é˜…è¯»åŸæ–‡</a>')
                    html_parts.append('</div>')
                
                html_parts.append('</div>')
                topic_index += 1
        
        if not merged_topics and not unique_topics:
            html_parts.append('<p>ä»Šæ—¥æš‚æ— æ–°é—»å†…å®¹ã€‚</p>')
        
        html_parts.append('</body>')
        html_parts.append('</html>')
        
        return '\n'.join(html_parts)

    def generate_html_file(self, merged_topics, unique_topics):
        """ç”Ÿæˆç‹¬ç«‹çš„HTMLæ–‡ä»¶"""
        html_content = self._generate_html_content(merged_topics, unique_topics)
        
        with open(self.html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {self.html_file}")

def main():
    aggregator = DateFilteredRSSAggregator()
    
    # è·å–ä»Šæ—¥æ–‡ç« 
    articles = aggregator.fetch_and_filter_articles()
    
    if not articles:
        print("ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œç”Ÿæˆç©ºçš„RSSæ–‡ä»¶")
        aggregator.generate_rss([], [])
        aggregator.generate_html_file([], [])
    else:
        # èšç±»å’Œå»é‡
        merged_topics, unique_topics = aggregator._cluster_and_filter_topics(articles)
        
        # ç”ŸæˆRSSå’ŒHTMLæ–‡ä»¶
        aggregator.generate_rss(merged_topics, unique_topics)
        aggregator.generate_html_file(merged_topics, unique_topics)
    
    print(f"RSSèšåˆå»é‡å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {aggregator.output_file} å’Œ {aggregator.html_file}")

if __name__ == "__main__":
    main()